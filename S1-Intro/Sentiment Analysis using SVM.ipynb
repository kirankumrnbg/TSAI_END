{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment Analysis using SVM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM4BKy4zcZ/u7+x/ScYuANY"},"kernelspec":{"name":"stanfordnlp","display_name":"NLPenv"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ljVmH4TqfkZs"},"source":["# SVM based Sentiment Analysis\n","Let's perform a SVM based Sentiment Analysis based on Support a Vector Machine Model on Twitter Sentiments of US Airline passengers.\n","\n","**Fill in the blanks**"]},{"cell_type":"code","metadata":{"id":"tF_pfwune6tY"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\BlueFlames\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lxmlNote: you may need to restart the kernel to use updated packages.\n","WARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n","You should consider upgrading via the 'D:\\Python\\envs\\stanfordnlp\\python.exe -m pip install --upgrade pip' command.\n","\n","  Downloading lxml-4.6.1-cp37-cp37m-win_amd64.whl (3.5 MB)\n","Installing collected packages: lxml\n","Successfully installed lxml-4.6.1\n"]}],"source":["# pip install lxml"]},{"cell_type":"markdown","metadata":{"id":"Vcu9OCGWf8Kr"},"source":["## Import Libraries"]},{"cell_type":"code","metadata":{"id":"xJQkq7NJft9P"},"source":["import numpy as np\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","from nltk.tokenize import TweetTokenizer\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","from sklearn.pipeline import make_pipeline, Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer, accuracy_score, f1_score\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUojZ_-Ef0uO"},"source":["## Import data\n"]},{"cell_type":"code","metadata":{"id":"JX3uxVzUf1bw"},"source":["data = pd.read_csv(\"Tweets_Airline.csv\")\n","data.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n","0  570306133677760513           neutral                        1.0000   \n","1  570301130888122368          positive                        0.3486   \n","2  570301083672813571           neutral                        0.6837   \n","3  570301031407624196          negative                        1.0000   \n","4  570300817074462722          negative                        1.0000   \n","\n","  negativereason  negativereason_confidence         airline  \\\n","0            NaN                        NaN  Virgin America   \n","1            NaN                     0.0000  Virgin America   \n","2            NaN                        NaN  Virgin America   \n","3     Bad Flight                     0.7033  Virgin America   \n","4     Can't Tell                     1.0000  Virgin America   \n","\n","  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n","0                    NaN     cairdin                 NaN              0   \n","1                    NaN    jnardino                 NaN              0   \n","2                    NaN  yvonnalynn                 NaN              0   \n","3                    NaN    jnardino                 NaN              0   \n","4                    NaN    jnardino                 NaN              0   \n","\n","                                                text tweet_coord  \\\n","0                @VirginAmerica What @dhepburn said.         NaN   \n","1  @VirginAmerica plus you've added commercials t...         NaN   \n","2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n","3  @VirginAmerica it's really aggressive to blast...         NaN   \n","4  @VirginAmerica and it's a really big bad thing...         NaN   \n","\n","               tweet_created tweet_location               user_timezone  \n","0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n","1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n","2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n","3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n","4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>airline_sentiment</th>\n      <th>airline_sentiment_confidence</th>\n      <th>negativereason</th>\n      <th>negativereason_confidence</th>\n      <th>airline</th>\n      <th>airline_sentiment_gold</th>\n      <th>name</th>\n      <th>negativereason_gold</th>\n      <th>retweet_count</th>\n      <th>text</th>\n      <th>tweet_coord</th>\n      <th>tweet_created</th>\n      <th>tweet_location</th>\n      <th>user_timezone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>570306133677760513</td>\n      <td>neutral</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>cairdin</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica What @dhepburn said.</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:35:52 -0800</td>\n      <td>NaN</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>570301130888122368</td>\n      <td>positive</td>\n      <td>0.3486</td>\n      <td>NaN</td>\n      <td>0.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica plus you've added commercials t...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:59 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570301083672813571</td>\n      <td>neutral</td>\n      <td>0.6837</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>yvonnalynn</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:48 -0800</td>\n      <td>Lets Play</td>\n      <td>Central Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>570301031407624196</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Bad Flight</td>\n      <td>0.7033</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica it's really aggressive to blast...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:36 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570300817074462722</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Can't Tell</td>\n      <td>1.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica and it's a really big bad thing...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:14:45 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"-BZpfoYfgDkQ"},"source":["## We take only the tweets we are very confident with. We use the BeautifulSoup library to process html encoding present in some tweets."]},{"cell_type":"code","metadata":{"id":"LwFYL4JYgET1"},"source":["data_clean = data.copy()\n","data_clean = data_clean[data_clean['airline_sentiment_confidence'] > 0.65]\n","data_clean['text_clean'] = data_clean['text'].apply(lambda x: BeautifulSoup(x, \"lxml\").text)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zNpo6rKtgIjz"},"source":["## For simplicity we are going to distinguish two cases: tweets with negative sentiment and tweets with non-negative sentiment"]},{"cell_type":"code","metadata":{"id":"AhzEtW7qgJFZ"},"source":["data_clean['sentiment'] = data_clean['airline_sentiment'].apply(--Fill--) #Hint: Assign 1 to negative class and 0 to rest\n","data_clean = data_clean.loc[:, ['text_clean', 'sentiment']]\n","data_clean.head()"],"execution_count":5,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-5-cf7b7b4a864c>, line 1)","traceback":["\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-cf7b7b4a864c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    data_clean['sentiment'] = data_clean['airline_sentiment'].apply(--Fill--) #Hint: Assign 1 to negative class and 0 to rest\u001b[0m\n\u001b[1;37m                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"goWgpeISgVjb"},"source":["## We split the data into training and testing set:"]},{"cell_type":"code","metadata":{"id":"hb3xJkm1gWEV"},"source":["train, test = train_test_split(data_clean, test_size=0.2, random_state=1)\n","X_train = train['text_clean'].values\n","X_test = test['text_clean'].values\n","y_train = train['sentiment']\n","y_test = test['sentiment']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RUVhKrXJggFe"},"source":["## Preprocessing the Data"]},{"cell_type":"code","metadata":{"id":"bt-UPdxNgg1l"},"source":["def tokenize(text): \n","    tknzr = TweetTokenizer()\n","    return tknzr.tokenize(text)\n","\n","def stem(doc):\n","    return (stemmer.stem(w) for w in analyzer(doc))\n","\n","en_stopwords = set(stopwords.words(\"english\")) \n","\n","vectorizer = CountVectorizer(\n","    analyzer = 'word',\n","    tokenizer = tokenize,\n","    lowercase = True,\n","    ngram_range=(1, 1),\n","    stop_words = en_stopwords)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tIubNDdAglWc"},"source":["## We are going to use cross validation and grid search to find good hyperparameters for our SVM model. We need to build a pipeline."]},{"cell_type":"code","metadata":{"id":"10SpNhNlgmVm"},"source":["kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_GbUy4Vgrq3"},"source":["np.random.seed(1)\n","\n","pipeline_svm = make_pipeline(vectorizer, --Fill--(probability=True, \n","                                             kernel= --Fill--, \n","                                             class_weight= --Fill--)) #Hint : Linear kernel with balanced class weights\n","\n","grid_svm = GridSearchCV(pipeline_svm,\n","                    param_grid = {'svc__C': [0.01, 0.1, 1]}, \n","                    cv = kfolds,\n","                    scoring=\"roc_auc\",\n","                    verbose=1,   \n","                    n_jobs=-1) \n","\n","grid_svm.fit(X_train, y_train)\n","grid_svm.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NdSXvoPg0s-"},"source":["print(grid_svm.best_params_)\n","print(grid_svm.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2fifQEugzHr"},"source":["## Let's see how the model (with the best hyperparameters) works on the test data:"]},{"cell_type":"code","metadata":{"id":"j1vY0Ruag382"},"source":["def report_results(model, X, y):\n","    pred_proba = model.predict_proba(X)[:, 1]\n","    pred = model.predict(X)        \n","\n","    auc = roc_auc_score(y, pred_proba)\n","    acc = accuracy_score(y, pred)\n","    f1 = f1_score(y, pred)\n","    prec = precision_score(y, pred)\n","    rec = recall_score(y, pred)\n","    result = {'auc': auc, 'f1': f1, 'acc': acc, 'precision': prec, 'recall': rec}\n","    return result\n","\n","report_results(grid_svm.best_estimator_, X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Wk71TaNg7qY"},"source":["## ROC Curve"]},{"cell_type":"code","metadata":{"id":"efe0yTp1g8aw"},"source":["def get_roc_curve(model, X, y):\n","    pred_proba = model.predict_proba(X)[:, 1]\n","    fpr, tpr, _ = roc_curve(y, pred_proba)\n","    return fpr, tpr\n","\n","fpr, tpr = get_roc_curve(grid_svm.best_estimator_, X_test, y_test)\n","plt.figure(figsize=(14,8))\n","plt.plot(fpr, tpr, color=\"red\")\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Roc curve')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cr8-f-N1hAQx"},"source":["## Prediction"]},{"cell_type":"code","metadata":{"id":"pVWXH80shBOb"},"source":["pred = grid_svm.predict([\"flying with @united is always a great experience.\"])\n","print('negative' if pred == np.array([1]) else 'not negative')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chGlA_QmhC9Q"},"source":["pred = grid_svm.predict([\"flying with @united is always a great experience. If you don't lose your luggage\"])\n","print('negative' if pred == np.array([1]) else 'not negative')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AA5lve07hJzf"},"source":["**It easily distinguishes the text based on context!!**"]},{"cell_type":"code","metadata":{"id":"PnnVO1LOhQA7"},"source":[""],"execution_count":null,"outputs":[]}]}